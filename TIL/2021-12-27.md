# 2021-12-27 프로그래머스 데브코스 DAY 17

__오늘 배운 것__

- 데브코스
	- flask
    - REST API

- 스터디
	- 머신러닝의 종류
    - 누락값 처리
    - 드롭아웃

__깨달은 것__

flask는 db 연결하는 것을 미리 예습을 했는데도 나 혼자 해보려니 아직 어렵다... 좀 쉬었다가 다시 해볼 생각이다.

다만 스터디에서 많은 것을 배웠다. 특히 누락값 처리와 드롭아웃에 대해 깊게 조사를 했더니 몰랐던 사실들이 많았다.

# 누락된 값 다루기 (141p)



__데이터가 누락되는 종류__

- 무작위로 누락: 변수의 종류와 상관없는 누락
- 다른 변수와 관련있는 누락: 해당 변수가 아닌 다른 변수의 영향을 받아서 누락된 경우 (ex. 설문조사의 __뒷면__이 있는 지 모르고 응답하지 않아서 누락된 경우)
- 해당 변수와 관련있는 누락: 누락된 해당 변수에 관련해서 누락된 경우(어떤 설문조사에서 일부 질문에 **정치적인 성향** 등의 이유로 채우지 않았을 경우)

- 첫번째, 두번째는 제거하는 것이 좋지만, 세번째 경우 단순 제거하면 모델이 편향 될 가능성이 있다.



__누락값 해결책__

- 삭제: 데이터 버리기(row) or 변수 버리기(col) 

  - 단점: 모델이 편향될 수 있으며, 중요한 변수가 버려질 수 있음

  

- 대표값(mean, median, mode)으로 대체

  - 단점: 대체한 변수가 다른 변수와 상관관계가 있을 때, 상관관계를 고려하지 않음, 데이터 전체의 분산이 줄어들게 됨(평균), 데이터 전체에 편향이 생김(최빈값)

  

- 다중 대체법(multiple imputation, MICE)

  - 과정

    1. 대체하고자 하는 누락값을 비우고, 다른 누락값은 대표값으로 채운다.
    2. 다른 변수들을 X로하고, 대체하고자 하는 누락값이 속하는 변수를 Y로하는 선형회귀모델을 이용해 누락값을 채운다.
    3. 다른 누락값에 대해서도 똑같이 적용한다.
    4. 대체 전 값과 대체 후의 값의 차이가 0이 되도록 이 과정을 반복한다.

    

- KNN 사용, 변수를 통해 가까운 데이터 K개를 골라 이 데이터들의 평균을 사용함

  - 단점: 오래걸림, outlier에 민감함, 변수의 scale을 조절해줘야 함

출처: https://subinium.github.io/missing-data-handling/



# 계층적 샘플링

- 책 내용: 테스트 데이터에만 누락값이 있는 경우 훈련 샘플의 일부를 여러 번 복사해서 테스트 데이터에서 빠질 것 같은 특성을 제거하라고 한다.
- 처음에 테스트셋을 나눌 때부터 균일하게 나누는 방법은 없을까?

__계층적 샘플링__

- 전체 데이터셋을 대표할 수 있도록 계층의 분포가 균일하게 전체 데이터에서 test set을 샘플링하는 방법
- sklearn의 StratifiedShuffleSplit 객체를 이용하면 데이터를 계층적 샘플링할 수 있다.

```python
from sklearn.model_selection import StratifiedShuffleSplit

# 훈련 셋과 테스트 셋으로(n_splits=1), 테스트 셋의 사이즈는 20%로(test_size=0.2) 샘플링
split = StratifiedShuffleSplit(n_splits=1, test_size=0.2)

# housing 데이터의 ['income_cat'] 열을 기준으로 계층적 샘플링을 진행
for train_index, test_index in split.split(housing, housing["income_cat"]):
    strat_train_set = housing.loc[train_index]
    strat_test_set = housing.loc[test_index]
```



- 혹은 train_test_split 함수의 stratify 매개변수를 이용할 수 있다.

```python
from sklearn.model_selection import train_test_split

train, test = train_test_split(housing, test_size=0.2, stratify=housing['income_cat'])
```



# 드롭아웃(150p)



__드롭아웃의 효과__

- 네트워크를 간단하게 해준다(규제 작용)
- 매 iteration마다 무작위로 뉴런을 없애므로, 마치 bagging같이 계속 새로운 모델이 만들어지며 error를 평균화하는 효과를 준다. 이는 bagging과 마찬가지로 일반화했을 때 에러를 줄여준다.
- 특히 한정된 데이터가 있을 때 효과가 좋다, 충분히 큰 데이터가 있을 때에는 오히려 학습 성능을 낮출 수 있다.
- 데이터의 feature가 많지만 데이터의 수가 한정되어 있는 computer vision에 많이 사용된다.



__드롭아웃 비율(dropout rate)__

- 0 - 아무것도 드롭아웃되지 않음, 1 - 모두 드롭아웃됨
- 보통 0.2~0.5 사이의 값을 선택한다.
- output layer는 드롭아웃되지 않아야하며 input layer는 0.1과 같이 낮은 dropout rate를 가져야 한다.
- 케라스의 dropout layer는 0으로 버려지지 않은 input은 1/(1-dropout)을 곱해 모든 input의 합이 변하지 않도록한다고 한다.

https://keras.io/api/layers/regularization_layers/dropout/



__알아두어야 할 것__

- gradient checking(역전파가 잘되고 있는지 알아볼 때)는 dropout을 하지 말아야 한다.
- 드롭아웃은 train시에만 적용되고 test시엔 적용되지 않는다.
- test시에는 output layer에 연결된 가중치들에 (1 - dropout rate)를 곱하거나, 마지막 hidden layer의 출력값에 (1- dropout rate)를 곱한다.

참고1: https://towardsdatascience.com/coding-neural-network-dropout-3095632d25ce

참고2: https://datascience.stackexchange.com/questions/44293/how-does-dropout-work-during-testing-in-neural-network
